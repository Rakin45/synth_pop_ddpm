{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "def setup_logging(run_name):\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    os.makedirs(os.path.join(\"models\", run_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(\"results\", run_name), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fred's encoder code\n",
    "class DescreteEncoder:\n",
    "    def __init__(self, duration: int = 1440, step_size: int = 10):\n",
    "        self.duration = duration\n",
    "        self.step_size = step_size\n",
    "        self.steps = duration // step_size\n",
    "        self.index_to_acts = {}\n",
    "        self.acts_to_index = {}\n",
    "\n",
    "    def encode(self, data: pd.DataFrame):\n",
    "        # Create mappings from activity to index and vice versa\n",
    "        self.index_to_acts = {i: a for i, a in enumerate(data.act.unique())}\n",
    "        self.acts_to_index = {a: i for i, a in self.index_to_acts.items()}\n",
    "        \n",
    "        # Create a new DataFrame for encoded data\n",
    "        encoded_data = data.copy()\n",
    "        encoded_data['act'] = encoded_data['act'].map(self.acts_to_index)\n",
    "        return encoded_data\n",
    "    \n",
    "    def decode(self, encoded_image_grid) -> pd.DataFrame:\n",
    "        if isinstance(encoded_image_grid, torch.Tensor):\n",
    "            encoded_image_grid = encoded_image_grid.numpy()\n",
    "        \n",
    "        decoded = []\n",
    "        for pid in range(encoded_image_grid.shape[0]):\n",
    "            sequence = encoded_image_grid[pid]\n",
    "            current_act = None\n",
    "            act_start = None\n",
    "            \n",
    "            for time_step, act_index in enumerate(sequence):\n",
    "                # If the activity changes or it's the end of the day, record the activity\n",
    "                if act_index != current_act and current_act is not None:\n",
    "                    act_end = time_step * self.step_size\n",
    "                    decoded.append({\n",
    "                        \"pid\": pid,\n",
    "                        \"act\": self.index_to_acts[current_act],\n",
    "                        \"start\": act_start,\n",
    "                        \"end\": act_end\n",
    "                    })\n",
    "                    act_start = time_step * self.step_size\n",
    "                # If the activity changes, update the current activity\n",
    "                if act_index != current_act:\n",
    "                    current_act = act_index\n",
    "                    act_start = time_step * self.step_size\n",
    "            \n",
    "            # Add the last activity of the day if the day ended with an activity\n",
    "            if current_act is not None and act_start is not None:\n",
    "                decoded.append({\n",
    "                    \"pid\": pid,\n",
    "                    \"act\": self.index_to_acts[current_act],\n",
    "                    \"start\": act_start,\n",
    "                    \"end\": self.duration\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(decoded, columns=[\"pid\", \"act\", \"start\", \"end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and encode it\n",
    "data = pd.read_csv('./data/nts_population.csv')\n",
    "encoder = DescreteEncoder()\n",
    "encoded_data = encoder.encode(data)\n",
    "\n",
    "# Function to convert encoded data into an image grid\n",
    "def create_image_grid(encoded_data, encoder):\n",
    "    # Map pid to sequential indices starting from 0\n",
    "    pid_to_index = {pid: index for index, pid in enumerate(encoded_data['pid'].unique())}\n",
    "    \n",
    "    num_people = len(pid_to_index)\n",
    "    time_steps = encoder.steps\n",
    "    grid = np.zeros((num_people, time_steps))\n",
    "    \n",
    "    for _, row in encoded_data.iterrows():\n",
    "        pid_index = pid_to_index[row['pid']]\n",
    "        act_index = row['act']\n",
    "        start_step = row['start'] // encoder.step_size\n",
    "        end_step = row['end'] // encoder.step_size\n",
    "        grid[pid_index, start_step:end_step] = act_index\n",
    "    \n",
    "    return grid\n",
    "\n",
    "image_grid = create_image_grid(encoded_data, encoder)\n",
    "tensor_image_grid = torch.tensor(image_grid)\n",
    "torch.save(tensor_image_grid, './data/image_grid.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect mappings\n",
    "print(\"Index to Activities Mapping:\")\n",
    "print(encoder.index_to_acts)\n",
    "print(\"\\nActivities to Index Mapping:\")\n",
    "print(encoder.acts_to_index)\n",
    "\n",
    "# Calculate vocabulary size\n",
    "vocab_size = len(encoder.index_to_acts)\n",
    "print(\"\\nVocabulary Size:\", vocab_size)\n",
    "\n",
    "# Compare vocabulary size to expected size\n",
    "expected_vocab_size = 8  # Assuming 'ntoken' is the expected vocabulary size\n",
    "if vocab_size != expected_vocab_size:\n",
    "    print(f\"Warning: Vocabulary size ({vocab_size}) does not match expected size ({expected_vocab_size})\")\n",
    "else:\n",
    "    print(\"Vocabulary size matches expected size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_grid(image_grid, title = 'Activity Sequence (Training)' ):    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(image_grid, aspect='auto', cmap=\"tab10\")\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Individuals')\n",
    "    plt.show()\n",
    "plot_image_grid(image_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ActivityDataset(TensorDataset):\n",
    "    def __init__(self, sequences, context_size=2):\n",
    "        self.data = []\n",
    "        for sequence in sequences:\n",
    "            for i in range(context_size, len(sequence) - context_size):\n",
    "                context = [sequence[i + j] for j in range(-context_size, context_size + 1) if j != 0]\n",
    "                target = sequence[i]\n",
    "                self.data.append((torch.tensor(context), torch.tensor(target)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.to(device)  # Ensure inputs are on the correct device\n",
    "        embedded = self.embeddings(inputs).mean(dim=1)\n",
    "        out = self.linear(embedded)\n",
    "        return out\n",
    "\n",
    "def train_model(dataset, model, optimizer, epochs=10, device=\"cuda\"):\n",
    "    model.train()\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for context, target in DataLoader(dataset, batch_size=128, shuffle=True):\n",
    "            context, target = context.to(device), target.to(device)  # Ensure data is on the correct device\n",
    "            model.zero_grad()\n",
    "            log_probs = model(context)\n",
    "            loss = loss_function(log_probs, target.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss / len(dataset)}')\n",
    "\n",
    "# Assuming tensor_image_grid is already defined and is a tensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = ActivityDataset(tensor_image_grid.long(), context_size=2)\n",
    "CBOW = CBOWModel(vocab_size=8, embedding_dim=6).to(device)\n",
    "optimizer = optim.Adam(CBOW.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "train_model(dataset, CBOW, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = CBOW.embeddings.weight.data\n",
    "\n",
    "activities_to_idx = {\n",
    "    'home': 0, 'shop': 1, 'work': 2, 'escort': 3, 'other': 4, 'education': 5, 'visit': 6, 'medical': 7\n",
    "}\n",
    "\n",
    "# Print the vector for each activity\n",
    "for activity, idx in activities_to_idx.items():\n",
    "    vector = embedding_weights[idx]\n",
    "    print(f\"Vector for {activity}: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_image_grid = tensor_image_grid.to(device)\n",
    "tensor_image_grid = tensor_image_grid.long()\n",
    "\n",
    "# Apply embeddings\n",
    "embedded_sequences = CBOW.embeddings(tensor_image_grid)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(embedded_sequences)\n",
    "\n",
    "# Initialize DataLoader\n",
    "batch_size = 64  \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)  \n",
    "\n",
    "\n",
    "for embedded_batch in dataloader:\n",
    "    print(embedded_batch[0].shape)  #([batch_size, 144, embedding_dim])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=25, beta_start=0.0001, beta_end=0.02, sequence_length=144, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = self.schedule_noise().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "    def schedule_noise(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "    \n",
    "    def noise_embeddings(self, embeddings, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1. - self.alpha_hat[t])[:, None, None]\n",
    "\n",
    "        epsilon = torch.randn_like(embeddings)\n",
    "\n",
    "        noisy_embeddings = sqrt_alpha_hat * embeddings + sqrt_one_minus_alpha_hat * epsilon\n",
    "\n",
    "        # Debug: Check for NaNs\n",
    "        if torch.isnan(noisy_embeddings).any():\n",
    "            print(\"NaNs detected in noisy_embeddings\")\n",
    "            print(\"sqrt_alpha_hat:\", sqrt_alpha_hat)\n",
    "            print(\"sqrt_one_minus_alpha_hat:\", sqrt_one_minus_alpha_hat)\n",
    "            print(\"epsilon:\", epsilon)\n",
    "\n",
    "        return noisy_embeddings.float(), epsilon.float()\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, embedding_dim):\n",
    "        logging.info(f\"Sampling {n} new sequences...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, self.sequence_length, embedding_dim)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                predicted_noise = model(x, t)\n",
    "                \n",
    "                alpha = self.alpha[t][:, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None]\n",
    "                beta = self.beta[t][:, None, None]\n",
    "\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "\n",
    "                x = (1 / torch.sqrt(alpha)) * (x - ((1 - alpha) / torch.sqrt(1 - alpha_hat)) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "\n",
    "        model.train()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diffusion Sampling Algorithm](sampling.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import TransformerWithPositionalEncoding\n",
    "\n",
    "def setup_logging(run_name):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def train(model, diffusion, device):\n",
    "    run_name = \"first_run\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    learning_rate = 0.002\n",
    "    epochs = 150\n",
    "\n",
    "    setup_logging(run_name)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    mse = nn.MSELoss() \n",
    "    logger = SummaryWriter(os.path.join(\"runs\", run_name))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        logging.info(f\"Starting epoch {epoch}\")\n",
    "        epoch_loss = 0.0\n",
    "        for batch_idx, (x,) in enumerate(tqdm(dataloader)):\n",
    "            embedded_x = x.to(device) \n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            timesteps = torch.randint(0, diffusion.noise_steps, (embedded_x.size(0),), device=device)\n",
    "            embedded_x = embedded_x.float()\n",
    "            noisy_x, _ = diffusion.noise_embeddings(embedded_x, timesteps)\n",
    "            predicted_x = model(noisy_x, timesteps)\n",
    "\n",
    "            loss = mse(predicted_x, noisy_x)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()  \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Log training loss to TensorBoard every 10 batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                logger.add_scalar(\"Loss/train\", loss.item(), epoch * len(dataloader) + batch_idx)\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        logging.info(f\"Epoch {epoch} Average Loss: {avg_epoch_loss}\")\n",
    "        logger.add_scalar(\"Loss/epoch_avg_train\", avg_epoch_loss, epoch)\n",
    "\n",
    "    logger.close()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = TransformerWithPositionalEncoding(feature_size=6, num_layers=3, max_time_steps=144).to(device)\n",
    "diffusion = Diffusion(noise_steps=25, beta_start=0.0001, beta_end=0.02, sequence_length=144, device=\"cuda\")\n",
    "\n",
    "train(model, diffusion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_embeddings_to_activities(embedded_sequences, CBOW):\n",
    "\n",
    "    embeddings = CBOW.embeddings.weight.data\n",
    "    \n",
    "    # Normalize embeddings to unit vectors\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "    \n",
    "    embedded_sequences = F.normalize(embedded_sequences, p=2, dim=-1)\n",
    "\n",
    "    # Calculate dot products (shape: [n * sequence_length, NUM_ACTIVITIES])\n",
    "    # Embedded_sequences are 3D ([n, sequence_length, embedding_dim]) and embeddings is 2D,\n",
    "    # we need to reshape embedded_sequences appropriately\n",
    "    n, sequence_length, embedding_dim = embedded_sequences.size()\n",
    "    embedded_sequences_flat = embedded_sequences.view(-1, embedding_dim)  # Reshape for matmul\n",
    "    dot_products = torch.matmul(embedded_sequences_flat, embeddings.T)\n",
    "    \n",
    "    # The resulting indices are the activities (shape: [n * sequence_length])\n",
    "    _, activities = torch.max(dot_products, dim=1)\n",
    "\n",
    "    # Reshape back to original sequence format ([n, sequence_length])\n",
    "    activities = activities.view(n, sequence_length)\n",
    "    \n",
    "    return activities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After generating new sequences using the diffusion model\n",
    "generated_sequences = diffusion.sample(model = model, n=32, embedding_dim=6)\n",
    "\n",
    "# Map the generated sequences back to discrete activities\n",
    "discrete_activities = map_embeddings_to_activities(generated_sequences, CBOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode each activity index back to its label\n",
    "decoded_sequences = [[encoder.index_to_acts[index.item()] for index in sequence] for sequence in discrete_activities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decoded = pd.DataFrame(decoded_sequences, columns=[f\"Step {i}\" for i in range(len(decoded_sequences[0]))])\n",
    "df_decoded.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_grid(discrete_activities.cpu(), title = 'Activity Sequence (Sampled)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
